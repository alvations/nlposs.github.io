---
title: "Invited Speakers"
bg: '#89CFBA'
color: black
fa-icon: microphone
---


<h3>
	<strong>Open-Source Software's Responsibility to Science</strong><br/>
	<a href="http://joelnothman.com/">Joel Nothman</a>, University of Sydney
</h3>

Open-source software makes sophisticated technologies available to a wide audience. Arguably, most people applying language processing and machine learning techniques rely on popular open source tools targeted at these applications. Users may themselves be incapable of implementing the underlying algorithms. Users may or may not have extensive training to critically conduct experiments with these tools.

As maintainers of popular scientific software, we should be aware of our user base, and consider the ways in which our software design and documentation can lead or mislead users with respect to scientific best practices. In this talk, I will present some examples of these risks, primarily drawn from my experience developing Scikit-learn. For example: How can we help users avoid data leakage in cross-validation? How can we help users report precisely which algorithm or metric was used in an experiment?

Volunteer OSS maintainers have limited ability to see and manage these risks, and need the scientific community's assistance to get things right in design, implementation and documentation.

#### ***Bio*** 

<img src="img/joel-nothman.jpg" alt="Joel Nothman" align="left" style="margin-right: 32px; margin-bottom: 16px;" />

Joel began contributing to the Scientific Python ecosystem of open-source software as a research student at the University of Sydney in 2008. He has since made substantial contributions to the NLTK, Scipy, Pandas and IPython packages among others, but presently puts most of his open-source energies into maintaining Scikit-learn, a popular machine learning toolkit. Joel works as a data science research engineer at the University of Sydney, who fund some of his open-source development efforts. He completed his PhD on event reference there in 2014, and has been teaching their Natural Language Processing unit since 2016.


<hr/>

<h3>
	<strong>Stanford CoreNLP: 15 Years of Developing Academic Open Source Software</strong><br/>
	<a href="https://nlp.stanford.edu/manning/">Christopher Manning</a>, Stanford University
</h3>

My students and I at the Stanford NLP Group started releasing academic open source NLP software relatively early, in 2002. Over the years, the status and popularity of particular tools, and, since 2010, of the integrated Stanford CoreNLP offering has continually grown. It is not only used as a reliable tool — and easy mark to beat — in academic NLP, but it is widely used across government, non-profits, startups, and large companies. In this talk, I give my reflections on building academic open source software: what is required, what is important, and what is not so important; what we did right and what we did wrong; how a software project can be maintained long-term in such a context, how it adds to and detracts from doing academic research, narrowly defined; and how the world has changed and what the prospects are for the future.

#### ***Bio*** 

<img src="img/christopher-manning.jpg" alt="Christopher Manning" align="left" style="margin-right: 32px; margin-bottom: 16px;" />
 
 Prof. Christopher Manning is the Thomas M. Siebel Professor in Machine Learning at Stanford University, in the Departments of Computer Science and Linguistics. He works on software that can intelligently process, understand, and generate human language material. He is a leader in applying Deep Learning to Natural Language Processing, with well-known research on the GloVe model of word vectors, Tree Recursive Neural Networks, sentiment analysis, neural network dependency parsing, neural machine translation, and deep language understanding. <a href="javascript: $('#talk1-bio').toggle();">(Read more ...)</a> <span id="talk1-bio" hidden>His computational linguistics work also covers probabilistic models of language, natural language inference and multilingual language processing, including being a principal developer of Stanford Dependencies and Universal Dependencies. Manning has coauthored leading textbooks on statistical approaches to Natural Language Processing (NLP) (Manning and Schütze 1999) and information retrieval (Manning, Raghavan, and Schütze, 2008), as well as linguistic monographs on ergativity and complex predicates. Manning is an ACM Fellow, a AAAI Fellow, an ACL Fellow, and Past President of the ACL. Research of his has won ACL, Coling, EMNLP, and CHI Best Paper Awards. He has a B.A. (Hons) from The Australian National University, a Ph.D. from Stanford in 1994, and he held faculty positions at Carnegie Mellon University and the University of Sydney before returning to Stanford. He is a member of the Stanford NLP group (@stanfordnlp) and manages development of the Stanford CoreNLP software.</span>

<hr/>

<h3>
	<strong>Building Marian (A Fast Neural Machine Translation in C++)</strong><br/>
	<a href="http://emjotde.github.io/">Marcin Junczys-Dowmunt</a>, Microsoft Translation 
</h3>

This talk will share some lessons learnt from building Marian. More details coming up...

#### ***Bio*** 

<img src="img/marcin.jpeg" alt="Marcin Junczys-Dowmunt" width=160 height=160 align="left" style="margin-right: 32px; margin-bottom: 16px;" />

Marcin Junczys-Dowmunt is a Principal NLP Scientist in the Machine Translation team at Microsoft -- Redmond since January 2018. Before joining Microsoft, He was an Assistant Professor at the Adam Mickiewicz University in Poznan, Poland, and a visiting researcher in the MT group at the University of Edinburgh. He collaborated for many years with the World Intellectual Property Organization and the United Nations, helping with the development of their in-house statistical and neural machine translation systems. His main research interests are neural machine translation, automatic post-editing and grammatical error correction. Most of my open-source activity is being eaten up by my NMT pet-project [Marian](http://github.com/marian-nmt/marian).


<!--
<h3>
	<strong>Reflections on Running spaCy: Commercial Open-source NLP</strong><br/>
	<a href="https://explosion.ai/">Matthew Honnibal and Ines Montani</a>, Explosion AI
</h3>

In this talk, I'll share some lessons we've learned from running spaCy, the fastest-growing library for Natural Language Processing in Python, and provide our perspective on how to make commercial open-source work for both users and developers. Every open-source project must strike a balance between the responsibilities and control of the maintainers, and the responsibilities and control of the users. Understanding and communicating the motivations for publishing software under an open-source license can put less pressure on maintainers, and help users select projects appropriate for their requirements.

#### ***Bio*** 

<img src="img/ines-montani.jpg" alt="Ines Montani" align="left" style="margin-right: 32px; margin-bottom: 16px;" />

Ines Montani is the lead developer of Prodigy, and a core contributor to spaCy. Although a full-stack developer, Ines has particular expertise in front-end development, having started building websites when she was 11. Before founding Explosion AI, she was a freelance developer and strategist, using her four years executive experience in ad sales and digital marketing.

<br/><br/>

<img src="img/matt-honnibal.jpg" alt="Matthew Honnibal" align="left" style="margin-right: 32px; margin-bottom: 16px;" />

Matthew Honnibal began his research career as a linguist, completing his PhD in 2009 on lexicalised parsing with Combinatory Categorial Grammar, before working on incremental speech parsing. These days he is best known as a software engineer, for his work on the spaCy NLP library. He grew up in Sydney, lives in Berlin, and still misses CCG.

-->
